{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fff89d8e-e853-4191-b425-d86b1281ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version:  2.5.1\n",
      "tiktoken version:  0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "print(\"torch version: \", version(\"torch\"))\n",
    "print(\"tiktoken version: \", version(\"tiktoken\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4326bbff-f286-4af2-b8b6-a42f754e0226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of charactoer:  748126\n",
      "﻿The Project Gutenberg eBook of Pride and Prejudice\n",
      "    \n",
      "This ebook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg Lic\n"
     ]
    }
   ],
   "source": [
    "file_path = './data/Jane-Austen-Pride_and_Prejudice-pg1342.txt'\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of charactoer: \", len(raw_text))\n",
    "print(raw_text[:299])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9e1e36-2c12-49f7-bdb4-096817bc4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = 'Hello, world. This, is a test.'\n",
    "result = re.split(r'(\\s)', text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9413f42-34d4-46d0-84bd-81d8897c1edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(r'([,.]|\\s)', text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4611b17e-16c5-4a59-a29d-625a672838a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
     ]
    }
   ],
   "source": [
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09549dc0-c1c9-4243-ad14-c998320b4d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world. Is this-- a test?\"\n",
    "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "result = [item.strip() for item in result if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eea8f5ef-43cc-4ff2-858a-016fdfc048e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153971\n"
     ]
    }
   ],
   "source": [
    "preprocessed = re.split(r'([,.;:?_!\"()\\\"]|--|\\s)', raw_text)\n",
    "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "print(len(preprocessed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63af4d72-ea9c-43d0-9140-143b3e3e7249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\ufeffThe', 'Project', 'Gutenberg', 'eBook', 'of', 'Pride', 'and', 'Prejudice', 'This', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'United', 'States', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', '.', 'You', 'may', 'copy', 'it', ',', 'give', 'it']\n"
     ]
    }
   ],
   "source": [
    "print( preprocessed[:45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf7d80c5-c423-4171-ba22-b3f374a1b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8254\n"
     ]
    }
   ],
   "source": [
    "# Create a Vocabulary\n",
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33db050e-0236-41ba-82c8-5fbfc3d887bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('!', 0)\n",
      "('#1342]', 1)\n",
      "('$1', 2)\n",
      "('$5', 3)\n",
      "('&', 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "('***', 7)\n",
      "('*/', 8)\n",
      "(',', 9)\n",
      "('-', 10)\n",
      "('--', 11)\n",
      "('.', 12)\n",
      "('/*', 13)\n",
      "('//www', 14)\n",
      "('000', 15)\n",
      "('1', 16)\n",
      "('10', 17)\n",
      "('108', 18)\n",
      "('113', 19)\n",
      "('118', 20)\n",
      "('12', 21)\n",
      "('132', 22)\n",
      "('139', 23)\n",
      "('143', 24)\n",
      "('146', 25)\n",
      "('148', 26)\n",
      "('15', 27)\n",
      "('1500', 28)\n",
      "('154', 29)\n",
      "('156', 30)\n",
      "('15th', 31)\n",
      "('161', 32)\n",
      "('166', 33)\n",
      "('168', 34)\n",
      "('175', 35)\n",
      "('177', 36)\n",
      "('1796', 37)\n",
      "('18', 38)\n",
      "('181', 39)\n",
      "('1813', 40)\n",
      "('189', 41)\n",
      "('1894', 42)\n",
      "('1894]', 43)\n",
      "('18th', 44)\n",
      "('194', 45)\n",
      "('198', 46)\n",
      "('1998', 47)\n",
      "('2', 48)\n",
      "('20%', 49)\n",
      "('200', 50)\n",
      "('2001', 51)\n"
     ]
    }
   ],
   "source": [
    "vocab = {token:integer for integer, token in enumerate(all_words)}\n",
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i > 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b02bb66c-edac-404f-b308-52aba96b22ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but I think she would have made a pretty good journey even in a black one.\n",
      "[1827, 501, 7173, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 1703, 5235, 12]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocablary):    # vocabulary\n",
    "        self.str_to_int = vocablary                                         #A\n",
    "        self.int_to_str = {i:s for s,i in vocablary.items()}                #B\n",
    "\n",
    "    def encode(self, text):                                                 #C\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):                                                  #D\n",
    "        return ' '.join([self.int_to_str[i] for i in ids])   \n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)                     #E\n",
    "        return text\n",
    "\n",
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "text = \"but I think she would have made a pretty good journey even in a black one.\"\n",
    "print(text)\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6f0d4106-f799-4bb8-a145-5c34825ea1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but I think she would have made a pretty good journey even in a black one .\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffc4f13a-7477-45bd-8a01-98be98490792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All number of the all_tokens :  8254\n",
      "8256\n"
     ]
    }
   ],
   "source": [
    "# Add the special context token\n",
    "all_tokens = sorted(list(set(preprocessed)))\n",
    "print(\" All number of the all_tokens : \", len(all_tokens))\n",
    "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
    "\n",
    "vocab = {token:integer for integer, token in enumerate(all_tokens)}\n",
    "print(len(vocab.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "532d0518-48f8-450d-9974-7456f914d16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('“this', 8226)\n",
      "('“though', 8227)\n",
      "('“to', 8228)\n",
      "('“undoubtedly', 8229)\n",
      "('“very', 8230)\n",
      "('“was', 8231)\n",
      "('“we', 8232)\n",
      "('“were', 8233)\n",
      "('“what', 8234)\n",
      "('“when', 8235)\n",
      "('“where', 8236)\n",
      "('“whether', 8237)\n",
      "('“which', 8238)\n",
      "('“while', 8239)\n",
      "('“who', 8240)\n",
      "('“why', 8241)\n",
      "('“will', 8242)\n",
      "('“without', 8243)\n",
      "('“yes', 8244)\n",
      "('“you', 8245)\n",
      "('“your', 8246)\n",
      "('“‘After', 8247)\n",
      "('“‘I', 8248)\n",
      "('“‘When', 8249)\n",
      "('“’Tis', 8250)\n",
      "('”', 8251)\n",
      "('•', 8252)\n",
      "('\\ufeffThe', 8253)\n",
      "('<|endoftext|>', 8254)\n",
      "('<|unk|>', 8255)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(list(vocab.items())[-30:]):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50684313-aae6-4e31-9398-be3adc0ad610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> but I overhaul she would have made a pretty good journey even in a pink one.\n"
     ]
    }
   ],
   "source": [
    "class SimpleTokenizerV2:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [token.strip() for token in tokens if token.strip()]\n",
    "        #print(\" Tokens = \", tokens)\n",
    "        preprocessed = [item if item in self.str_to_int else \"<|unk|>\" for item in tokens]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        #print(\" idx = \", ids)\n",
    "        return ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])                   #B\n",
    "        text = re.sub(r'\\s+([,.;:?!\"()\\'])', r'\\1', text)\n",
    "        return text\n",
    "\n",
    "text1 = \"Hello, do you like tea?\"\n",
    "text2 = \"but I overhaul she would have made a pretty good journey even in a pink one.\"\n",
    "text = \" <|endoftext|> \".join((text1, text2))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb94852a-0cdf-4d09-b502-8a142a51519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokens =  ['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'but', 'I', 'overhaul', 'she', 'would', 'have', 'made', 'a', 'pretty', 'good', 'journey', 'even', 'in', 'a', 'pink', 'one', '.']\n",
      " idx =  [8255, 9, 2894, 7890, 4656, 7098, 142, 8254, 1827, 501, 8255, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 8255, 5235, 12]\n",
      "[8255, 9, 2894, 7890, 4656, 7098, 142, 8254, 1827, 501, 8255, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 8255, 5235, 12]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV2(vocab)\n",
    "print(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b4fd5b2-9ba8-4f2e-8e2f-1056fc3c4a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text =  Hello, do you like tea? <|endoftext|> but I overhaul she would have made a pretty good journey even in a pink one.\n",
      " Tokens =  ['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'but', 'I', 'overhaul', 'she', 'would', 'have', 'made', 'a', 'pretty', 'good', 'journey', 'even', 'in', 'a', 'pink', 'one', '.']\n",
      " idx =  [8255, 9, 2894, 7890, 4656, 7098, 142, 8254, 1827, 501, 8255, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 8255, 5235, 12]\n",
      " Encoding =  [8255, 9, 2894, 7890, 4656, 7098, 142, 8254, 1827, 501, 8255, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 8255, 5235, 12]\n",
      " length =  25\n",
      " Tokens =  ['Hello', ',', 'do', 'you', 'like', 'tea', '?', '<|endoftext|>', 'but', 'I', 'overhaul', 'she', 'would', 'have', 'made', 'a', 'pretty', 'good', 'journey', 'even', 'in', 'a', 'pink', 'one', '.']\n",
      " idx =  [8255, 9, 2894, 7890, 4656, 7098, 142, 8254, 1827, 501, 8255, 6571, 7860, 3928, 4752, 1033, 5720, 3761, 4494, 3206, 4193, 1033, 8255, 5235, 12]\n",
      " Decoded =  but I think she would have made a pretty good journey even in a black one.\n",
      " decoded length =  74\n"
     ]
    }
   ],
   "source": [
    "print(\"text = \", text)\n",
    "encoded = tokenizer.encode(text)\n",
    "print(\" Encoding = \", encoded)\n",
    "print(\" length = \", len(encoded))\n",
    "\n",
    "decoded = tokenizer.decode(tokenizer.encode(text)) \n",
    "print(\" Decoded = \", decoded)\n",
    "print(\" decoded length = \", len(decoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346532c-7f08-467e-8a06-9a9437214b61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs224n",
   "language": "python",
   "name": "cs224n"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
